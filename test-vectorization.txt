Testing document vectorization with full content extraction:

Document ID 203: Ownermaual (Hyundai CRETA Manual)
- Current stored content: 10,000 characters (truncated)
- LlamaParse extracted: 898,946 characters
- Expected vector coverage: Should be ~300 chunks instead of 4

Document ID 204: Financial Report
- Current stored content: 10,000 characters (truncated) 
- LlamaParse extracted: 29,810 characters
- Expected vector coverage: Should be ~10 chunks instead of 4

Issue: content field was being truncated to 10,000 characters before storing in database, 
causing incomplete vector coverage for large documents.

Solution: Remove substring truncation in openai.ts processDocument function.
Result: Full content will be stored and properly chunked for comprehensive vector search.