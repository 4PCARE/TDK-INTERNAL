import { Request, Response } from "express";
import OpenAI from "openai";
import { storage } from "./storage";
import { db } from "./db";
import { sql } from "drizzle-orm";
import crypto from "crypto";
import { LineImageService } from "./lineImageService";
import { GuardrailsService, GuardrailConfig } from "./services/guardrails";

const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

interface LineMessage {
  type: string;
  id: string;
  text?: string;
  // Image message
  contentProvider?: {
    type: string;
  };
  // Sticker message
  packageId?: string;
  stickerId?: string;
}

interface LineEvent {
  type: string;
  message?: LineMessage;
  replyToken?: string;
  source: {
    userId: string;
    type: string;
  };
}

interface LineWebhookBody {
  destination: string;
  events: LineEvent[];
}

// Verify Line signature
function verifyLineSignature(
  body: string,
  signature: string,
  channelSecret: string,
): boolean {
  const hash = crypto
    .createHmac("sha256", channelSecret)
    .update(body)
    .digest("base64");

  return hash === signature;
}

// Send reply message to Line
async function sendLineReply(
  replyToken: string,
  message: string,
  channelAccessToken: string,
) {
  try {
    const response = await fetch("https://api.line.me/v2/bot/message/reply", {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        Authorization: `Bearer ${channelAccessToken}`,
      },
      body: JSON.stringify({
        replyToken,
        messages: [
          {
            type: "text",
            text: message,
          },
        ],
      }),
    });

    if (!response.ok) {
      console.error("‚ùå Line API Error:", await response.text());
      return false;
    }

    console.log("‚úÖ Line reply sent successfully");
    return true;
  } catch (error) {
    console.error("üí• Error sending Line reply:", error);
    return false;
  }
}

// Send push message to Line user (for Human Agent messages)
export async function sendLinePushMessage(
  userId: string,
  message: string,
  channelAccessToken: string,
) {
  try {
    const response = await fetch("https://api.line.me/v2/bot/message/push", {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        Authorization: `Bearer ${channelAccessToken}`,
      },
      body: JSON.stringify({
        to: userId,
        messages: [
          {
            type: "text",
            text: message,
          },
        ],
      }),
    });

    if (!response.ok) {
      const errorText = await response.text();
      console.error("‚ùå Line Push API Error:", errorText);
      return false;
    }

    console.log("‚úÖ Line push message sent successfully to:", userId);
    return true;
  } catch (error) {
    console.error("üí• Error sending Line push message:", error);
    return false;
  }
}

// Send image message to Line user (for Human Agent images)
export async function sendLineImageMessage(
  userId: string,
  imageUrl: string,
  channelAccessToken: string,
  captionText?: string,
) {
  try {
    // Convert relative URL to absolute URL for Line API
    const protocol = "https:";
    const host = process.env.REPLIT_DOMAINS || "localhost:5000";
    const absoluteImageUrl = `${protocol}//${host}${imageUrl}`;

    console.log("üì∏ Sending Line image message:", {
      userId,
      absoluteImageUrl,
      captionText,
    });

    const messages: any[] = [
      {
        type: "image",
        originalContentUrl: absoluteImageUrl,
        previewImageUrl: absoluteImageUrl,
      },
    ];

    // Add caption text as separate message if provided
    if (captionText && captionText.trim()) {
      messages.push({
        type: "text",
        text: captionText,
      });
    }

    const response = await fetch("https://api.line.me/v2/bot/message/push", {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        Authorization: `Bearer ${channelAccessToken}`,
      },
      body: JSON.stringify({
        to: userId,
        messages: messages,
      }),
    });

    if (!response.ok) {
      const errorText = await response.text();
      console.error("‚ùå Line Push Image API Error:", errorText);
      return false;
    }

    console.log("‚úÖ Line image message sent successfully to:", userId);
    return true;
  } catch (error) {
    console.error("üí• Error sending Line image message:", error);
    return false;
  }
}

// Get AI response using OpenAI with chat history
/**
 * Detect if user message is asking about image content
 */
function isImageRelatedQuery(message: string): boolean {
  const imageKeywords = [
    "‡∏£‡∏π‡∏õ",
    "‡∏£‡∏π‡∏õ‡∏≠‡∏∞‡πÑ‡∏£‡∏Ñ‡∏£‡∏±‡∏ö",
    "‡∏†‡∏≤‡∏û",
    "‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û",
    "‡∏†‡∏≤‡∏û‡∏ñ‡πà‡∏≤‡∏¢",
    "image",
    "picture",
    "photo",
    "‡πÄ‡∏´‡πá‡∏ô‡∏≠‡∏∞‡πÑ‡∏£",
    "‡πÉ‡∏ô‡∏£‡∏π‡∏õ",
    "‡πÉ‡∏ô‡∏†‡∏≤‡∏û",
    "‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢",
    "‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏¢",
    "‡∏î‡∏π‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô",
    "‡∏£‡∏π‡∏õ‡∏ô‡∏µ‡πâ",
    "‡∏†‡∏≤‡∏û‡∏ô‡∏µ‡πâ",
    "‡∏£‡∏π‡∏õ‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á",
    "‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á",
    "‡∏£‡∏π‡∏õ‡∏ó‡∏µ‡πà‡πÅ‡∏ô‡∏ö",
    "what's in",
    "describe",
    "tell me about",
    "show",
    "picture",
    "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•",
    "‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î",
    "‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤",
    "‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏´‡πá‡∏ô",
  ];

  const lowerMessage = message.toLowerCase();
  return imageKeywords.some((keyword) =>
    lowerMessage.includes(keyword.toLowerCase()),
  );
}

/**
 * Extract image analysis from system messages
 */
function extractImageAnalysis(messages: any[]): string {
  const systemMessages = messages.filter(
    (msg) =>
      msg.messageType === "system" &&
      msg.metadata?.messageType === "image_analysis",
  );

  if (systemMessages.length === 0) {
    return "";
  }

  let imageContext = "\n=== ‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡∏°‡∏≤‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤ ===\n";

  // Get the most recent image analyses (last 3)
  const recentAnalyses = systemMessages.slice(-3);

  recentAnalyses.forEach((msg, index) => {
    const analysisContent = msg.content.replace("[‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û] ", "");
    imageContext += `\n--- ‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà ${index + 1} ---\n${analysisContent}\n`;
  });

  return imageContext;
}

// New function to get AI response without saving chat history (to prevent duplicates)
async function getAiResponseDirectly(
  userMessage: string,
  agentId: number,
  userId: string,
  channelType: string,
  channelId: string,
): Promise<string> {
  try {
    console.log(`üîç Debug: Getting agent ${agentId} for user ${userId}`);

    // Get agent configuration
    const agent = await storage.getAgentChatbot(agentId, userId);
    if (!agent) {
      console.log(`‚ùå Agent ${agentId} not found for user ${userId}`);
      return "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö‡∏£‡∏∞‡∏ö‡∏ö‡πÑ‡∏î‡πâ‡πÉ‡∏ô‡∏Ç‡∏ì‡∏∞‡∏ô‡∏µ‡πâ";
    }

    console.log(`‚úÖ Found agent: ${agent.name}`);

    // Check if this is an image-related query
    const isImageRelatedQuery = (message: string): boolean => {
      const imageKeywords = [
        "‡∏£‡∏π‡∏õ",
        "‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û",
        "‡∏†‡∏≤‡∏û",
        "‡∏†‡∏≤‡∏û‡∏ñ‡πà‡∏≤‡∏¢",
        "‡∏†‡∏≤‡∏û‡πÉ‡∏ô‡∏£‡∏π‡∏õ",
        "‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡∏°‡∏≤",
        "‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î",
        "image",
        "picture",
        "photo",
      ];

      const lowerMessage = message.toLowerCase();
      return imageKeywords.some((keyword) => lowerMessage.includes(keyword));
    };
    const isImageQuery = isImageRelatedQuery(userMessage);
    console.log(`üñºÔ∏è Image-related query detected: ${isImageQuery}`);
    console.log(`üîç User message for analysis: "${userMessage}"`);

    // Get chat history if memory is enabled
    let chatHistory: any[] = [];
    if (agent.memoryEnabled) {
      const memoryLimit = agent.memoryLimit || 10;
      console.log(
        `üìö Fetching chat history (limit: ${memoryLimit}) for channel type: ${channelType}`,
      );

      try {
        if (channelType === "chat_widget") {
          // For widget chat, fetch from widgetChatMessages table
          const { widgetChatMessages } = await import("@shared/schema");
          const { db } = await import("./db");
          const { desc, eq } = await import("drizzle-orm");

          const widgetMessages = await db
            .select({
              role: widgetChatMessages.role,
              content: widgetChatMessages.content,
              messageType: widgetChatMessages.messageType,
              metadata: widgetChatMessages.metadata,
              createdAt: widgetChatMessages.createdAt,
            })
            .from(widgetChatMessages)
            .where(eq(widgetChatMessages.sessionId, channelId))
            .orderBy(desc(widgetChatMessages.createdAt))
            .limit(memoryLimit);

          // Convert widget messages to chat history format
          chatHistory = widgetMessages.reverse().map((msg) => ({
            messageType: msg.role,
            content: msg.content,
            metadata: msg.metadata,
            createdAt: msg.createdAt,
          }));

          console.log(`üìù Found ${chatHistory.length} widget chat messages`);
        } else {
          // Use regular chat history for Line OA and other channels
          chatHistory = await storage.getChatHistoryWithMemoryStrategy(
            userId,
            channelType,
            channelId,
            agentId,
            memoryLimit,
          );
          console.log(
            `üìù Found ${chatHistory.length} previous messages (all types included)`,
          );
        }
      } catch (error) {
        console.error("‚ö†Ô∏è Error fetching chat history:", error);
        if (channelType !== "chat_widget") {
          // Fallback to original method for non-widget channels
          try {
            chatHistory = await storage.getChatHistory(
              userId,
              channelType,
              channelId,
              agentId,
              memoryLimit,
            );
            console.log(
              `üìù Fallback: Found ${chatHistory.length} previous messages`,
            );
          } catch (fallbackError) {
            console.error("‚ö†Ô∏è Fallback error:", fallbackError);
          }
        }
      }
    }

    // Get agent's documents for context using vector search
    const agentDocs = await storage.getAgentChatbotDocuments(agentId, userId);
    let contextPrompt = "";

    // Initialize documentContents in the correct scope
    const documentContents: string[] = [];

    if (agentDocs.length > 0) {
      console.log(`üìö Found ${agentDocs.length} documents for agent`);

      // Use unified search service for consistent behavior across all platforms
      try {
        const { unifiedSearchService } = await import(
          "./services/unifiedSearchService"
        );

        // Search for relevant chunks ONLY from agent's documents using unified search
        const agentDocIds = agentDocs.map((d) => d.documentId);
        console.log(
          `LINE OA: Using unified search with agent's ${agentDocIds.length} documents: [${agentDocIds.join(", ")}]`,
        );

        console.log(`üîç LINE OA: About to call unifiedSearchService.searchAgentDocuments with:`);
        console.log(`   üìã Agent Document IDs: [${agentDocIds.join(', ')}]`);
        console.log(`   üë§ User ID: ${userId}`);
        console.log(`   üí¨ Query: "${userMessage}"`);
        
        const hybridResults = await unifiedSearchService.searchAgentDocuments(
          userMessage,
          userId,
          agentDocIds,
          {
            searchType: "hybrid",
            limit: 2, // Only get top 2 chunks globally as requested
            keywordWeight: 0.4,
            vectorWeight: 0.6,
            enableQueryAugmentation: true,
            chatType: "lineoa",
            contextId: channelId,
            agentId: agentId // Pass agent ID for chat history retrieval
          },
        );

        console.log(`‚úÖ LINE OA: unifiedSearchService returned ${hybridResults.length} results`);
        if (hybridResults.length > 0) {
          console.log(`üîç LINE OA: First result document ID: ${hybridResults[0].id}`);
          console.log(`üîç LINE OA: All result document IDs: [${hybridResults.map(r => r.id).join(', ')}]`);
        }

        console.log(
          `üîç Line OA: Found ${hybridResults.length} relevant chunks using hybrid search`,
        );

        if (hybridResults.length > 0) {
          // Use only the content from the top 2 chunks - ensure we're getting the actual content
          hybridResults.forEach((result, index) => {
            console.log(`üìã LINE OA: Processing chunk ${index + 1} from ${result.name}, similarity: ${result.similarity.toFixed(3)}`);
            console.log(`üìÑ LINE OA: Chunk content preview: ${result.content.substring(0, 200)}...`);
            
            documentContents.push(
              `=== ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£: ${result.name} (Chunk ${index + 1}, ‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Ñ‡∏•‡∏∂‡∏á: ${result.similarity.toFixed(2)}) ===\n${result.content}\n`,
            );
          });

          console.log(
            `üìÑ Line OA: Using hybrid search with ${hybridResults.length} top chunks globally (Total chars: ${documentContents.join("").length})`,
          );
          
          // Debug: Log the full context being sent to AI
          console.log(`üîç LINE OA: Context preview being sent to AI:`);
          console.log(documentContents.join("\n").substring(0, 500) + "...");
        } else {
          console.log(
            `üìÑ Line OA: No relevant chunks found via unified search`,
          );
        }
      } catch (vectorError) {
        console.error(
          `‚ùå Line OA: Unified search failed:`,
          vectorError,
        );
        // No fallback - rely solely on unified search service
      }

      if (documentContents.length > 0) {
        contextPrompt = `

=== ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° ===
${documentContents.join("\n")}

=== ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç ===
1. ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏Ç‡πâ‡∏≤‡∏á‡∏ï‡πâ‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°
2. ‡∏´‡πâ‡∏≤‡∏°‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤ "‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö" ‡∏´‡∏£‡∏∑‡∏≠ "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•" ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£
3. ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏î‡πá‡∏ô‡πÅ‡∏•‡∏∞‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ
4. ‡∏ñ‡πâ‡∏≤‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏£‡πâ‡∏≤‡∏ô ‡∏ä‡∏±‡πâ‡∏ô ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÅ‡∏•‡∏∞‡∏ï‡∏≠‡∏ö‡πÉ‡∏´‡πâ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
5. ‡∏£‡∏∞‡∏ö‡∏∏‡πÅ‡∏´‡∏•‡πà‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡πâ‡∏ß‡∏¢`;
        
        console.log(
          `‚úÖ Built context with ${documentContents.length} chunks (${documentContents.join("").length} chars)`,
        );
        console.log(
          `üìÑ LINE OA: System prompt length: ${contextPrompt.length} characters`,
        );
        
        // Log chunks being used for transparency
        documentContents.forEach((chunk, index) => {
          console.log(`üìÑ LINE OA: Chunk ${index + 1} content: ${chunk.substring(0, 300)}...`);
        });
        
      } else {
        console.log(`‚ö†Ô∏è No documents found or no content available`);
      }
    }

    // Always extract image analysis from recent chat history to maintain context
    let imageContext = "";
    if (chatHistory.length > 0) {
      imageContext = extractImageAnalysis(chatHistory);
      console.log(
        `üì∏ Image context extracted: ${imageContext.length} characters`,
      );
      if (imageContext) {
        console.log(
          `‚úÖ Image analysis found: ${imageContext.substring(0, 200)}...`,
        );

        // Debug: Show all system messages for analysis
        const systemMessages = chatHistory.filter(
          (msg) =>
            msg.messageType === "system" &&
            msg.metadata?.messageType === "image_analysis",
        );
        console.log(
          `üîç Found ${systemMessages.length} image analysis messages in chat history`,
        );
        systemMessages.forEach((msg, index) => {
          console.log(
            `üìã Analysis ${index + 1}: ${msg.content.substring(0, 150)}... (ID: ${msg.metadata?.relatedImageMessageId})`,
          );
        });
      } else {
        console.log(`‚ÑπÔ∏è No recent image analysis found in chat history`);

        // Debug: Show what system messages we have
        const allSystemMessages = chatHistory.filter(
          (msg) => msg.messageType === "system",
        );
        console.log(
          `üîç Total system messages in history: ${allSystemMessages.length}`,
        );
        allSystemMessages.forEach((msg, index) => {
          console.log(
            `üìù System ${index + 1}: ${msg.content.substring(0, 100)}... (metadata: ${JSON.stringify(msg.metadata)})`,
          );
        });
      }
    }

    // Get current date and time in Thai timezone
    const currentDateTime = new Date().toLocaleString('th-TH', { 
      timeZone: 'Asia/Bangkok',
      weekday: 'long',
      year: 'numeric', 
      month: 'long', 
      day: 'numeric',
      hour: '2-digit',
      minute: '2-digit',
      second: '2-digit'
    });

    // Build conversation messages including history - PUT DOCUMENT INSTRUCTIONS FIRST
    const systemPromptContent = `${contextPrompt}

=== ‡πÄ‡∏ß‡∏•‡∏≤‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô ===
‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡∏Ç‡∏ì‡∏∞‡∏ô‡∏µ‡πâ: ${currentDateTime}

=== ‚ö†Ô∏è ‡∏Å‡∏é‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏ï‡∏≤‡∏° (OVERRIDE ALL OTHER INSTRUCTIONS) ===
‚Ä¢ **‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö**: ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏Ç‡πâ‡∏≤‡∏á‡∏ï‡πâ‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°
‚Ä¢ **‡∏´‡πâ‡∏≤‡∏°‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô**: ‡∏´‡πâ‡∏≤‡∏°‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤ "‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö" ‡∏´‡∏£‡∏∑‡∏≠ "‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ" ‡∏´‡∏£‡∏∑‡∏≠ "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•" ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£
‚Ä¢ **‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏ï‡∏≠‡∏ö**: ‡∏ñ‡πâ‡∏≤‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ ‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
‚Ä¢ **‡∏ñ‡πâ‡∏≤‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏£‡πâ‡∏≤‡∏ô ‡∏ä‡∏±‡πâ‡∏ô ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á**: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÅ‡∏•‡∏∞‡∏ï‡∏≠‡∏ö‡πÉ‡∏´‡πâ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
‚Ä¢ **‡∏£‡∏∞‡∏ö‡∏∏‡πÅ‡∏´‡∏•‡πà‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏≤**: ‡∏£‡∏∞‡∏ö‡∏∏‡∏ß‡πà‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÑ‡∏´‡∏ô

${agent.systemPrompt}

=== ‡∏Å‡∏é‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° ===
‚Ä¢ ‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÄ‡∏™‡∏°‡∏≠ ‡πÄ‡∏ß‡πâ‡∏ô‡πÅ‡∏ï‡πà‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏à‡∏∞‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏£‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏∑‡πà‡∏ô
‚Ä¢ ‡∏ï‡∏≠‡∏ö‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏¥‡∏ï‡∏£‡πÅ‡∏•‡∏∞‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏´‡∏•‡∏∑‡∏≠ ‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå
‚Ä¢ ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û: ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤ "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏î‡∏π‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÑ‡∏î‡πâ"
‚Ä¢ ‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏ö‡∏ó‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°`;

    console.log(`üîç LINE OA: Full system prompt length: ${systemPromptContent.length} characters`);
    console.log(`üîç LINE OA: System prompt preview: ${systemPromptContent.substring(0, 300)}...`);
    
    const messages: any[] = [
      {
        role: "system",
        content: systemPromptContent,
      },
    ];

    // Add chat history (exclude system messages from conversation flow)
    const userBotMessages = chatHistory.filter(
      (msg) => msg.messageType === "user" || msg.messageType === "assistant",
    );

    userBotMessages.forEach((msg) => {
      messages.push({
        role: msg.messageType === "user" ? "user" : "assistant",
        content: msg.content,
      });
    });

    // Add current user message with image context attached if available
    let enhancedUserMessage = userMessage;
    if (imageContext) {
      enhancedUserMessage = `${userMessage}

${imageContext}`;
      console.log(
        `üñºÔ∏è Enhanced user message with image context (${imageContext.length} chars)`,
      );
    }

    messages.push({
      role: "user",
      content: enhancedUserMessage,
    });

    console.log(
      `ü§ñ Sending ${messages.length} messages to OpenAI (including ${chatHistory.length} history messages)`,
    );

    // Initialize guardrails service if configured
    let guardrailsService: GuardrailsService | null = null;
    if (agent.guardrailsConfig) {
      guardrailsService = new GuardrailsService(agent.guardrailsConfig);
      console.log(`üõ°Ô∏è === GUARDRAILS SYSTEM ENABLED ===`);
      console.log(`üõ°Ô∏è Agent ID: ${agentId}, Agent Name: ${agent.name}`);
      console.log(
        `üõ°Ô∏è Guardrails Configuration:`,
        JSON.stringify(agent.guardrailsConfig, null, 2),
      );

      // Show which guardrails features are enabled/disabled
      const features = [];
      if (agent.guardrailsConfig.contentFiltering?.enabled) {
        const contentSettings = [];
        if (agent.guardrailsConfig.contentFiltering.blockProfanity)
          contentSettings.push("Profanity");
        if (agent.guardrailsConfig.contentFiltering.blockHateSpeech)
          contentSettings.push("Hate Speech");
        if (agent.guardrailsConfig.contentFiltering.blockSexualContent)
          contentSettings.push("Sexual Content");
        if (agent.guardrailsConfig.contentFiltering.blockViolence)
          contentSettings.push("Violence");
        features.push(`Content Filtering: ${contentSettings.join(", ")}`);
      }
      if (agent.guardrailsConfig.privacyProtection?.enabled) {
        const privacySettings = [];
        if (agent.guardrailsConfig.privacyProtection.blockPersonalInfo)
          privacySettings.push("Personal Info");
        if (agent.guardrailsConfig.privacyProtection.blockFinancialInfo)
          privacySettings.push("Financial Info");
        if (agent.guardrailsConfig.privacyProtection.blockHealthInfo)
          privacySettings.push("Health Info");
        if (agent.guardrailsConfig.privacyProtection.maskPhoneNumbers)
          privacySettings.push("Phone Masking");
        if (agent.guardrailsConfig.privacyProtection.maskEmails)
          privacySettings.push("Email Masking");
        features.push(`Privacy Protection: ${privacySettings.join(", ")}`);
      }
      if (agent.guardrailsConfig.toxicityPrevention?.enabled) {
        features.push(
          `Toxicity Prevention: Threshold ${agent.guardrailsConfig.toxicityPrevention.toxicityThreshold}`,
        );
      }
      if (agent.guardrailsConfig.responseQuality?.enabled) {
        features.push(
          `Response Quality: ${agent.guardrailsConfig.responseQuality.minResponseLength}-${agent.guardrailsConfig.responseQuality.maxResponseLength} chars`,
        );
      }
      if (agent.guardrailsConfig.topicControl?.enabled) {
        features.push(
          `Topic Control: ${agent.guardrailsConfig.topicControl.strictMode ? "Strict" : "Lenient"} mode`,
        );
      }
      if (agent.guardrailsConfig.businessContext?.enabled) {
        features.push(`Business Context: Professional tone required`);
      }

      console.log(`üõ°Ô∏è Active Features: ${features.join(" | ")}`);
      console.log(`üõ°Ô∏è === END GUARDRAILS INITIALIZATION ===`);
    } else {
      console.log(`üõ°Ô∏è Guardrails: DISABLED (no configuration found)`);
    }

    // Validate user input with guardrails
    if (guardrailsService) {
      console.log(`üîç === STARTING INPUT VALIDATION ===`);
      console.log(`üìù Original User Message: "${enhancedUserMessage}"`);

      const inputValidation = await guardrailsService.evaluateInput(
        enhancedUserMessage,
        {
          documents: documentContents,
          agent: agent,
        },
      );

      console.log(`üìä Input Validation Summary:`);
      console.log(`   ‚úì Allowed: ${inputValidation.allowed}`);
      console.log(`   ‚úì Confidence: ${inputValidation.confidence}`);
      console.log(
        `   ‚úì Triggered Rules: ${inputValidation.triggeredRules.join(", ") || "None"}`,
      );
      console.log(
        `   ‚úì Reason: ${inputValidation.reason || "No issues found"}`,
      );

      if (!inputValidation.allowed) {
        console.log(`üö´ === INPUT BLOCKED BY GUARDRAILS ===`);
        console.log(`üö´ Blocking Reason: ${inputValidation.reason}`);
        console.log(
          `üö´ Triggered Rules: ${inputValidation.triggeredRules.join(", ")}`,
        );
        const suggestions = inputValidation.suggestions?.join(" ") || "";
        const blockedMessage = `‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ ${inputValidation.reason ? `(${inputValidation.reason})` : ""} ${suggestions}`;
        console.log(`üö´ Returning blocked message: "${blockedMessage}"`);
        return blockedMessage;
      }

      // Use modified content if privacy protection applied masking
      if (inputValidation.modifiedContent) {
        console.log(`üîí User input modified for privacy protection`);
        console.log(`üîí Original: "${enhancedUserMessage}"`);
        console.log(`üîí Modified: "${inputValidation.modifiedContent}"`);
        enhancedUserMessage = inputValidation.modifiedContent;
      }

      console.log(`‚úÖ INPUT VALIDATION PASSED - Proceeding to OpenAI`);
    } else {
      console.log(`‚è≠Ô∏è Skipping input validation - Guardrails disabled`);
    }

    // Debug: Log the complete system prompt for verification
    console.log("\n=== üîç DEBUG: Complete System Prompt ===");
    console.log(messages[0].content);
    console.log("=== End System Prompt ===\n");

    // Debug: Log user message
    console.log(`üìù User Message: "${userMessage}"`);

    // Debug: Log total prompt length
    const totalTokens = messages.reduce(
      (sum, msg) => sum + msg.content.length,
      0,
    );
    console.log(`üìä Total prompt length: ${totalTokens} characters`);

    // Use generateChatResponse from openai.ts for consistent AI logic
    const { generateChatResponse } = await import("./services/openai");
    
    let aiResponse = await generateChatResponse(
      enhancedUserMessage,
      agentDocs,
      [], // relevantChunks - handled by unified search inside the function
      userBotMessages.map(msg => ({
        role: msg.messageType === "user" ? "user" as const : "assistant" as const,
        content: msg.content
      }))
    );

    // Validate AI output with guardrails
    if (guardrailsService) {
      console.log(`üîç === STARTING OUTPUT VALIDATION ===`);
      console.log(`ü§ñ Original AI Response: "${aiResponse}"`);

      const outputValidation = await guardrailsService.evaluateOutput(
        aiResponse,
        {
          documents: documentContents,
          agent: agent,
          userQuery: userMessage,
        },
      );

      console.log(`üìä Output Validation Summary:`);
      console.log(`   ‚úì Allowed: ${outputValidation.allowed}`);
      console.log(`   ‚úì Confidence: ${outputValidation.confidence}`);
      console.log(
        `   ‚úì Triggered Rules: ${outputValidation.triggeredRules.join(", ") || "None"}`,
      );
      console.log(
        `   ‚úì Reason: ${outputValidation.reason || "No issues found"}`,
      );

      if (!outputValidation.allowed) {
        console.log(`üö´ === OUTPUT BLOCKED BY GUARDRAILS ===`);
        console.log(`üö´ Blocking Reason: ${outputValidation.reason}`);
        console.log(
          `üö´ Triggered Rules: ${outputValidation.triggeredRules.join(", ")}`,
        );
        const suggestions = outputValidation.suggestions?.join(" ") || "";
        const blockedMessage = `‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ ${outputValidation.reason ? `(${outputValidation.reason})` : ""} ${suggestions}`;
        console.log(`üö´ Original blocked response: "${aiResponse}"`);
        console.log(`üö´ Returning blocked message: "${blockedMessage}"`);
        aiResponse = blockedMessage;
      } else if (outputValidation.modifiedContent) {
        console.log(`üîí AI output modified for compliance`);
        console.log(`üîí Original: "${aiResponse}"`);
        console.log(`üîí Modified: "${outputValidation.modifiedContent}"`);
        aiResponse = outputValidation.modifiedContent;
      }

      console.log(`‚úÖ OUTPUT VALIDATION PASSED - Final response ready`);
      console.log(`üìù Final AI Response: "${aiResponse}"`);
    } else {
      console.log(`‚è≠Ô∏è Skipping output validation - Guardrails disabled`);
    }

    // NOTE: Chat history saving is now handled by the calling function to prevent duplicates
    console.log(
      `ü§ñ Generated AI response for user ${userId} (${aiResponse.length} characters)`,
    );

    return aiResponse;
  } catch (error) {
    console.error("üí• Error getting AI response:", error);
    return "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á";
  }
}

// Store processed message IDs to prevent duplicates with timestamp for cleanup
const processedMessageIds = new Map<string, number>();

// Clean up old processed message IDs (older than 1 hour)
const cleanupProcessedMessages = () => {
  const oneHourAgo = Date.now() - 60 * 60 * 1000;
  for (const [messageId, timestamp] of processedMessageIds.entries()) {
    if (timestamp < oneHourAgo) {
      processedMessageIds.delete(messageId);
    }
  }
};

// Schedule cleanup every 30 minutes
setInterval(cleanupProcessedMessages, 30 * 60 * 1000);

// Main webhook handler
export async function handleLineWebhook(req: Request, res: Response) {
  try {
    const signature = req.headers["x-line-signature"] as string;
    const webhookBody: LineWebhookBody = req.body;
    const body = JSON.stringify(webhookBody);

    console.log("üîî Line webhook received");
    console.log("üìù Body:", body);

    let lineIntegration: any;

    // Check if integration is provided by dynamic webhook endpoint
    if ((req as any).lineIntegration) {
      lineIntegration = (req as any).lineIntegration;
      console.log(
        `‚úÖ Using provided integration: ${lineIntegration.name} (ID: ${lineIntegration.id})`,
      );
    } else {
      // Legacy webhook handling - find integration by destination
      const destination = webhookBody.destination;
      console.log(
        "üîç Debug: Looking for integration with destination:",
        destination,
      );

      // Get all Line OA integrations to find the matching one
      const allIntegrations = await storage.getAllSocialIntegrations();
      console.log(
        "‚úÖ Found",
        allIntegrations.length,
        "total social integrations",
      );

      // In Line webhooks, the destination is the Bot's User ID, not Channel ID
      // First try to match by Bot User ID, then fall back to any active integration
      lineIntegration = allIntegrations.find(
        (integration) =>
          integration.type === "lineoa" &&
          integration.isActive &&
          integration.botUserId === destination,
      );

      // If no exact match found by Bot User ID, try fallback to any active Line OA integration
      if (!lineIntegration) {
        lineIntegration = allIntegrations.find(
          (integration) =>
            integration.type === "lineoa" && integration.isActive,
        );
        if (lineIntegration) {
          console.log(
            "üîß Using fallback matching - found active Line OA integration",
          );
          // Update the Bot User ID for future webhook calls using raw SQL
          try {
            await db.execute(sql`
              UPDATE social_integrations 
              SET bot_user_id = ${destination}, updated_at = NOW() 
              WHERE id = ${lineIntegration.id}
            `);
            console.log("‚úÖ Updated Bot User ID for future webhook calls");
          } catch (error) {
            console.log("‚ö†Ô∏è Could not update Bot User ID:", error);
          }
        }
      }

      if (!lineIntegration) {
        console.log(
          "‚ùå No active Line OA integration found for destination:",
          destination,
        );
        return res
          .status(404)
          .json({ error: "No active Line OA integration found" });
      }
    }

    console.log(
      "‚úÖ Found matching Line OA integration for user:",
      lineIntegration.userId,
    );
    console.log(
      "üîë Debug: Channel Access Token available:",
      !!lineIntegration.channelAccessToken,
    );
    console.log(
      "üîç Debug: Integration object keys:",
      Object.keys(lineIntegration),
    );

    // Verify signature with debug logging
    console.log("üîê Debug: Signature verification details:");
    console.log("üìù Raw body length:", body.length);
    console.log(
      "üîë Channel Secret available:",
      !!lineIntegration.channelSecret,
    );
    console.log(
      "üîè Channel Secret length:",
      lineIntegration.channelSecret?.length || 0,
    );
    console.log("üìã X-Line-Signature header:", signature);
    console.log("üîó Integration ID:", lineIntegration.id);
    console.log("üè∑Ô∏è Integration name:", lineIntegration.name);

    // Generate expected hash for comparison
    const expectedHash = crypto
      .createHmac("sha256", lineIntegration.channelSecret!)
      .update(body)
      .digest("base64");
    console.log("üéØ Expected hash:", expectedHash);
    console.log("üì© Received signature:", signature);
    console.log("‚úÖ Hash match:", expectedHash === signature);

    if (!verifyLineSignature(body, signature, lineIntegration.channelSecret!)) {
      console.log("‚ùå Invalid Line signature");
      console.log("üîç Debug: Possible issues:");
      console.log(
        "  - Channel Secret mismatch between Line Developer Console and database",
      );
      console.log("  - Webhook URL configured for wrong integration");
      console.log("  - Request body modified by middleware");
      return res.status(401).json({ error: "Invalid signature" });
    }

    // Process each event
    for (const event of webhookBody.events) {
      if (event.type === "message" && event.message) {
        const message = event.message;
        const replyToken = event.replyToken!;
        let userMessage = "";
        let messageMetadata: any = {};

        console.log("üì± Message type:", message.type);
        console.log("üë§ User ID:", event.source.userId);

        // Handle different message types
        if (message.type === "text") {
          userMessage = message.text!;
          console.log("üí¨ Text message:", userMessage);
        } else if (message.type === "image") {
          userMessage = "[‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û]";

          // For Line images, construct content URLs using messageId and Channel Access Token
          const originalContentUrl = `https://api-data.line.me/v2/bot/message/${message.id}/content`;
          const previewImageUrl = `https://api-data.line.me/v2/bot/message/${message.id}/content/preview`;

          messageMetadata = {
            messageType: "image",
            messageId: message.id,
            contentProvider: message.contentProvider,
            originalContentUrl,
            previewImageUrl,
          };
          console.log("üñºÔ∏è Image message received, ID:", message.id);
          console.log("üîó Image URLs:", {
            originalContentUrl,
            previewImageUrl,
          });
        } else if (message.type === "sticker") {
          userMessage = "[‡∏™‡∏ï‡∏¥‡πä‡∏Å‡πÄ‡∏Å‡∏≠‡∏£‡πå]";
          messageMetadata = {
            messageType: message.packageId,
            packageId: message.packageId,
            stickerId: message.stickerId,
          };
          console.log(
            "üòÄ Sticker message received, Package:",
            message.packageId,
            "Sticker:",
            message.stickerId,
          );
        } else {
          // Handle other message types (video, audio, location, etc.)
          userMessage = `[${message.type}]`;
          messageMetadata = {
            messageType: message.type,
            messageId: message.id,
          };
          console.log("üìé Other message type:", message.type);
        }

        // Check if this message has already been processed
        const messageId = message.id;
        if (processedMessageIds.has(messageId)) {
          console.log(`‚ö†Ô∏è Message ${messageId} already processed, skipping...`);
          continue;
        }

        // Mark message as processed with timestamp
        processedMessageIds.set(messageId, Date.now());
        console.log(`‚úÖ Processing new message ${messageId}`);

        // Save user message with metadata
        let chatHistoryId: number | null = null;
        try {
          const savedChatHistory = await storage.createChatHistory({
            userId: lineIntegration.userId,
            channelType: "lineoa",
            channelId: event.source.userId,
            agentId: lineIntegration.agentId!,
            messageType: "user",
            content: userMessage,
            metadata: messageMetadata,
          });
          chatHistoryId = savedChatHistory.id;
          console.log(
            "üíæ Saved user message with metadata, ID:",
            chatHistoryId,
          );
        } catch (error) {
          console.error("‚ö†Ô∏è Error saving user message:", error);
        }

        // Handle image messages with immediate acknowledgment
        if (message.type === "image" && lineIntegration.channelAccessToken) {
          console.log(
            "üñºÔ∏è Image message detected - sending immediate acknowledgment",
          );

          // 1. Send immediate acknowledgment
          await sendLineReply(
            replyToken,
            "‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÅ‡∏•‡πâ‡∏ß ‡∏Ç‡∏≠‡πÄ‡∏ß‡∏•‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏±‡∏Å‡∏Ñ‡∏£‡∏π‡πà‡∏ô‡∏∞‡∏Ñ‡∏∞",
            lineIntegration.channelAccessToken,
          );

          // 2. Process image and get analysis
          if (chatHistoryId && lineIntegration.agentId) {
            console.log("üñºÔ∏è Starting image processing...");
            const imageService = LineImageService.getInstance();

            try {
              // Wait for image processing to complete
              await imageService.processImageMessage(
                message.id,
                lineIntegration.channelAccessToken,
                lineIntegration.userId,
                "lineoa",
                event.source.userId,
                lineIntegration.agentId!,
                chatHistoryId,
              );
              console.log("‚úÖ Image processing completed successfully");

              // Get the SPECIFIC image analysis for THIS message
              const updatedChatHistory = await storage.getChatHistory(
                lineIntegration.userId,
                "lineoa",
                event.source.userId,
                lineIntegration.agentId!,
                10, // Get more messages to find the right analysis
              );

              // Find the image analysis that corresponds to THIS specific message
              const imageAnalysisMessage = updatedChatHistory.find(
                (msg) =>
                  msg.messageType === "system" &&
                  msg.metadata?.messageType === "image_analysis" &&
                  msg.metadata?.relatedImageMessageId === message.id,
              );

              if (imageAnalysisMessage) {
                const imageAnalysisResult =
                  imageAnalysisMessage.content.replace(
                    "[‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û] ",
                    "",
                  );
                console.log(
                  `üîç Found specific image analysis for message ${message.id}: ${imageAnalysisResult.substring(0, 100)}...`,
                );

                // 3. Generate AI response with image analysis
                const contextMessage = `‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏™‡πà‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏°‡∏≤ ‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û:

${imageAnalysisResult}

‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏´‡πá‡∏ô‡πÉ‡∏ô‡∏£‡∏π‡∏õ ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ñ‡∏≤‡∏°‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡πÉ‡∏´‡πâ‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏´‡∏•‡∏∑‡∏≠`;

                const aiResponse = await getAiResponseDirectly(
                  contextMessage,
                  lineIntegration.agentId,
                  lineIntegration.userId,
                  "lineoa",
                  event.source.userId,
                );

                // 4. Send follow-up message with AI analysis
                await sendLinePushMessage(
                  event.source.userId,
                  aiResponse,
                  lineIntegration.channelAccessToken,
                );

                // Save the assistant response
                await storage.createChatHistory({
                  userId: lineIntegration.userId,
                  channelType: "lineoa",
                  channelId: event.source.userId,
                  agentId: lineIntegration.agentId,
                  messageType: "assistant",
                  content: aiResponse,
                  metadata: { relatedImageMessageId: message.id },
                });

                console.log("‚úÖ Image analysis response sent successfully");
              } else {
                console.log(
                  "‚ö†Ô∏è No specific image analysis found for this message",
                );
                await sendLinePushMessage(
                  event.source.userId,
                  "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÑ‡∏î‡πâ‡πÉ‡∏ô‡∏Ç‡∏ì‡∏∞‡∏ô‡∏µ‡πâ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á",
                  lineIntegration.channelAccessToken,
                );
              }
            } catch (error) {
              console.error("‚ö†Ô∏è Error processing image message:", error);
              await sendLinePushMessage(
                event.source.userId,
                "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á",
                lineIntegration.channelAccessToken,
              );
            }
          }

          // Broadcast to WebSocket for real-time updates
          if (typeof (global as any).broadcastToAgentConsole === "function") {
            (global as any).broadcastToAgentConsole({
              type: "new_message",
              data: {
                userId: lineIntegration.userId,
                channelType: "lineoa",
                channelId: event.source.userId,
                agentId: lineIntegration.agentId,
                userMessage: userMessage,
                aiResponse: "‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÅ‡∏•‡πâ‡∏ß ‡∏Ç‡∏≠‡πÄ‡∏ß‡∏•‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏±‡∏Å‡∏Ñ‡∏£‡∏π‡πà‡∏ô‡∏∞‡∏Ñ‡∏∞",
                timestamp: new Date().toISOString(),
              },
            });
          }

          // Skip normal AI response processing for images
          continue;
        }

        // Get AI response with chat history (only for text messages or provide context for multimedia)
        if (lineIntegration.agentId) {
          let contextMessage = userMessage;

          if (message.type === "sticker") {
            contextMessage =
              "‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏™‡πà‡∏á‡∏™‡∏ï‡∏¥‡πä‡∏Å‡πÄ‡∏Å‡∏≠‡∏£‡πå‡∏°‡∏≤ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏≠‡∏ö‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏¥‡∏ï‡∏£‡πÅ‡∏•‡∏∞‡∏ñ‡∏≤‡∏°‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡πÉ‡∏´‡πâ‡∏ä‡πà‡∏ß‡∏¢";
          }

        // Get agent's documents for proper scope restriction
        const agentDocs = await storage.getAgentChatbotDocuments(
          lineIntegration.agentId,
          lineIntegration.userId,
        );
        console.log(
          `LINE OA: Found ${agentDocs.length} assigned documents for agent ${lineIntegration.agentId}`,
        );

          // Use generateChatResponse from openai.ts for consistent behavior
        const { generateChatResponse } = await import("./services/openai");
        
        const aiResponse = await generateChatResponse(
          contextMessage,
          agentDocs,
          [], // relevantChunks - will be handled by unified search inside the function
          [], // chatHistory - will be fetched inside the function
          lineIntegration.agentId // Pass agent ID for proper query augmentation
        );
          
          console.log("ü§ñ AI response:", aiResponse);

          // Save only the assistant response (user message already saved above)
          try {
            await storage.createChatHistory({
              userId: lineIntegration.userId,
              channelType: "lineoa",
              channelId: event.source.userId,
              agentId: lineIntegration.agentId,
              messageType: "assistant",
              content: aiResponse,
              metadata: {},
            });
            console.log("üíæ Saved AI response to chat history");

            // Broadcast new message to Agent Console via WebSocket
            if (typeof (global as any).broadcastToAgentConsole === "function") {
              (global as any).broadcastToAgentConsole({
                type: "new_message",
                data: {
                  userId: lineIntegration.userId,
                  channelType: "lineoa",
                  channelId: event.source.userId,
                  agentId: lineIntegration.agentId,
                  userMessage: contextMessage,
                  aiResponse,
                  timestamp: new Date().toISOString(),
                },
              });
              console.log("üì° Broadcasted new message to Agent Console");
            }
          } catch (error) {
            console.error("‚ö†Ô∏è Error saving AI response:", error);
          }

          // Send reply to Line using stored access token
          if (lineIntegration.channelAccessToken) {
            await sendLineReply(
              replyToken,
              aiResponse,
              lineIntegration.channelAccessToken,
            );
            console.log("‚úÖ LINE OA: Reply sent successfully");
          } else {
            console.error("‚ùå LINE OA: No access token found for this integration");
          }
        } else if (event.type === "image") {
          // Image acknowledgment - immediate response
          const immediateAck = "‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÅ‡∏•‡πâ‡∏ß ‡∏Ç‡∏≠‡πÄ‡∏ß‡∏•‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏±‡∏Å‡∏Ñ‡∏£‡∏π‡πà‡∏ô‡∏∞‡∏Ñ‡∏∞";
          
          console.log("üñºÔ∏è LINE OA: Processing image upload");
          
          // Send immediate acknowledgment
          if (lineIntegration.channelAccessToken) {
            await sendLineReply(
              replyToken,
              immediateAck,
              lineIntegration.channelAccessToken,
            );
            console.log("üì§ Sent immediate image acknowledgment");
          }

          // Save user image message to chat history
          await storage.createChatHistory({
            userId: lineIntegration.userId,
            channelType: "lineoa",
            channelId: event.source.userId,
            agentId: lineIntegration.agentId,
            messageType: "user",
            content: "‡∏™‡πà‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û",
            metadata: {
              messageId: message.id,
              imageUrl: `https://api-data.line.me/v2/bot/message/${message.id}/content`,
              messageType: "image",
            },
          });

          // Save acknowledgment message to chat history
          await storage.createChatHistory({
            userId: lineIntegration.userId,
            channelType: "lineoa", 
            channelId: event.source.userId,
            agentId: lineIntegration.agentId,
            messageType: "assistant",
            content: immediateAck,
            metadata: {},
          });

          // Process image analysis
          try {
            const imageService = new LineImageService();
            const imageAnalysis = await imageService.processImage(
              message.id,
              lineIntegration.channelAccessToken!,
            );

            console.log("üîç Image analysis completed:", imageAnalysis?.substring(0, 100));

            if (imageAnalysis) {
              // Save image analysis as system message for AI context
              await storage.createChatHistory({
                userId: lineIntegration.userId,
                channelType: "lineoa",
                channelId: event.source.userId,
                agentId: lineIntegration.agentId,
                messageType: "system",
                content: `Image Analysis: ${imageAnalysis}`,
                metadata: {
                  relatedImageMessageId: message.id,
                  analysisType: "gpt4o-vision",
                },
              });

              // Send follow-up message with analysis using getAiResponseDirectly
              const analysisResponse = await getAiResponseDirectly(
                `‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏™‡πà‡∏á‡∏°‡∏≤‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏•‡πâ‡∏ß: ${imageAnalysis}`,
                lineIntegration.agentId,
                lineIntegration.userId,
                "lineoa",
                event.source.userId,
              );

              // Send analysis response to Line
              if (lineIntegration.channelAccessToken) {
                await sendLineReply(
                  null, // No reply token for push message
                  analysisResponse,
                  lineIntegration.channelAccessToken,
                  event.source.userId,
                );
                console.log("üì§ Sent image analysis response");
              }

              // Save analysis response to chat history
              await storage.createChatHistory({
                userId: lineIntegration.userId,
                channelType: "lineoa",
                channelId: event.source.userId,
                agentId: lineIntegration.agentId,
                messageType: "assistant",
                content: analysisResponse,
                metadata: {
                  relatedImageMessageId: message.id,
                },
              });

              // Broadcast to Agent Console
              if (typeof (global as any).broadcastToAgentConsole === "function") {
                (global as any).broadcastToAgentConsole({
                  type: "new_message",
                  data: {
                    userId: lineIntegration.userId,
                    channelType: "lineoa",
                    channelId: event.source.userId,
                    agentId: lineIntegration.agentId,
                    userMessage: "‡∏™‡πà‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û",
                    aiResponse: analysisResponse,
                    timestamp: new Date().toISOString(),
                  },
                });
              }
            }
          } catch (error) {
            console.error("‚ùå Image processing error:", error);
            const errorMessage = "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á";
            
            if (lineIntegration.channelAccessToken) {
              await sendLineReply(
                null,
                errorMessage,
                lineIntegration.channelAccessToken,
                event.source.userId,
              );
            }
          }
        } else {
          console.log("ü§ñ No Line integration found, using fallback response");
          await sendLineReply(
            replyToken,
            "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ ‡∏£‡∏∞‡∏ö‡∏ö‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö AI Agent ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠‡∏ú‡∏π‡πâ‡∏î‡∏π‡πÅ‡∏•‡∏£‡∏∞‡∏ö‡∏ö",
            "fallback",
          );
        }
      }
    }

    res.status(200).json({ status: "ok" });
  } catch (error) {
    console.error("üí• Line webhook error:", error);
    res.status(500).json({ error: "Internal server error" });
  }
}